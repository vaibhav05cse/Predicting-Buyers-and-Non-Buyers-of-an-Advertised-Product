# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Random Forest
from sklearn.ensemble import RandomForestClassifier
classifier4 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier4.fit(X_train, y_train)
y_pred4 = classifier4.predict(X_test)
from sklearn.metrics import confusion_matrix
Rnad_forest_Con_Mat = confusion_matrix(y_test, y_pred4)

#Logsistic Regression
from sklearn.linear_model import LogisticRegression
classifier1 = LogisticRegression()
classifier1.fit(X_train, y_train)
y_pred1 = classifier1.predict(X_test)
Log_Regr_Con_Mat = confusion_matrix(y_test, y_pred1)

#KNN
from sklearn.neighbors import KNeighborsClassifier
classifier2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier2.fit(X_train, y_train)
y_pred2 = classifier2.predict(X_test)
KNN_Con_Mat = confusion_matrix(y_test, y_pred2)

#Naive Bayes
from sklearn.naive_bayes import GaussianNB
classifier3 = GaussianNB()
classifier3.fit(X_train, y_train)
y_pred3 = classifier3.predict(X_test)
Naive_Bayes_Con_Mat = confusion_matrix(y_test, y_pred3)

#This Section Plots CAP
total = len(y_test)
class_1_count = np.sum(y_test)
class_0_count = total - class_1_count
plt.figure(figsize = (10, 10))


# Random Model
plt.plot([0, total], [0, class_1_count], c = 'r', linestyle = '--', label = 'Random Model')

# Perfect Model
plt.plot([0, class_1_count, total], 
         [0, class_1_count, class_1_count], 
         c = 'grey', 
         linewidth = 2, 
         label = 'Perfect Model')

# Trained Model
probs = classifier3.predict_proba(X_test)
probs = probs[:, 1]
model_y = [y for _, y in sorted(zip(probs, y_test), reverse = True)]
y_values = np.append([0], np.cumsum(model_y))
x_values = np.arange(0, total + 1)
plt.plot(x_values, 
         y_values, 
         c = 'b', 
         label = 'Naive Bayes Model', 
         linewidth = 4)

# Point where vertical line will cut trained model
index = int((50*total / 100))

## 50% Verticcal line from x-axis
plt.plot([index, index], [0, y_values[index]], c ='g', linestyle = '--')

## Horizontal line to y-axis from prediction model
plt.plot([0, index], [y_values[index], y_values[index]], c = 'g', linestyle = '--')

class_1_observed = y_values[index] * 100 / max(y_values)

# Plot information
plt.xlabel('Audience of the Advertisement', fontsize = 16)
plt.ylabel('Buyers of the Product', fontsize = 16)
plt.title('Cumulative Accuracy Profile (Naive Bayes)', fontsize = 16)
plt.legend(loc = 'lower right', fontsize = 16)
